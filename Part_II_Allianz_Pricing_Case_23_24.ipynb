{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvxEHcMI1WSS"
   },
   "source": [
    "<center> <h1> <span style=\"color:black\"> Allianz Pricing Business Case  </h1> </center>\n",
    "<center> <h2> <span style=\"color:red\"> From data preparation to risk modeling </h1> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYTG-ttWBMQl"
   },
   "source": [
    "<center> <h2> <span style=\"color:red\"> Part II: modelling claim counts and amounts </h1> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbtRQa-UH51O"
   },
   "source": [
    "### 0. Authors and set-up\n",
    "This case was developed by Ana Pais and Charl Marais (Allianz Benelux), in the framework of the APC module on insurance analytics, UvA, 2023-2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRZDoOBBLROt"
   },
   "source": [
    "丘멆잺 **Initial considerations**\n",
    "\n",
    "This script assumes that you have access to the **Google Colab** platform or that you followed the steps mentioned on **initial_setup.pdf** to work locally on your laptops (if you do have the needed permissions from your employer).\n",
    "If none of the options is verified, you will not be able to proceed with the scripts provided, investigate the given data neither perform your own analysis.\n",
    "\n",
    "\n",
    "\n",
    "游닇 **Tasks**\n",
    "\n",
    "* Each task is numbered\n",
    "* Per each task, read the respective description of the task and execute the instuctions. For the majority of the tasks a cell with code is available that needs to be completed or run\n",
    "* The spaces to be completed are marked with a blank space _______________\n",
    "* If you do feel the need to create more cells you can click on the cell you are currently and press **B**. A new cell will be created under the active one. See for instance https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Notebook%20Basics.html\n",
    "* Along the tasks you can find the following emojis:\n",
    "    * 游닉: identifies some helpful hints refering to functions that can be used or code\n",
    "    * 游눫: identifies discussion points. Feel free to share your opinion!\n",
    "\n",
    "Feel free to reach out to us in case of questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sBuoB1_HLROu"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1FyZOrOLROv"
   },
   "source": [
    "## 1. Pre-processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFhKtMQdLROv",
    "tags": []
   },
   "source": [
    "The foundation of the technical premium assessment is to choose a fair value for the customer to pay based on their risk. To do this, the risk of the customer seeking insurance, must be estimated. The underlying assumption is that certain risk variables (or: features, covariates) are useful in explaining differences in the risk.\n",
    "\n",
    "\n",
    "How to create reasonable sub-groups (or: levels, bins) when working with categorical (or: factor), continuous or spatial variables is not an easy task. Some essential considerations to keep in mind is to find as much homogeneity as possible within a level or sub-group and to make sure every sub-group has a sufficient volume of observations (or: exposure to risk). This binning task can be done by relying on expert opinion or by resorting to some data-driven strategies to specify the sub-groups of a risk variable. The latter is the topic of the Henckaerts et al. (2018) paper in Scandinavian Actuarial Journal, see [link to publisher](https://www.tandfonline.com/doi/abs/10.1080/03461238.2018.1429300) or [paper on SSRN](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3052174).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuvGo4lcLROv"
   },
   "source": [
    "#### 1.1. Risk factors pre-processing\n",
    "\n",
    "游닇 Complete the path to load the merged policy-claim database, cfr. Part I of the Allianz Pricing Case 23-24\n",
    "\n",
    "游닇 Do the feature pre-processing for the following factor variables:\n",
    "\n",
    "* `Area`: {A, ..., F} -> {1, ..., 6} (has been set in `pol_clm_db.csv`)\n",
    "\n",
    "* `VehPower`: categorical feature, where we merge vehicle power groups bigger and equal to 9. In total, 6 classes based on expert opinion.\n",
    "\n",
    "* `VehAge`: 3 categorical classes [0, 1), [1, 10), (10, +inf) based on expert opinion\n",
    "\n",
    "* `DrivAge`: 7 categorical classes [18; 21), [21; 26), [26; 31), [31; 41), [41; 51), [51; 71), [71;+inf) based on expert opinion.\n",
    "\n",
    "* `BonusMalus`: continuous feature (capping at value 150)\n",
    "\n",
    "* `VehBrand`: categorical feature (in total 11 classes)\n",
    "\n",
    "* `VehGas`: binary feature\n",
    "\n",
    "* `Density`: log-density is chosen as continuous log-linear feature c(has been created in `pol_clm_db.csv' as `LogDensity`)\n",
    "\n",
    "* `Region`: categorical feature (in total 22 classes)\n",
    "\n",
    "游닉 Hint:\n",
    "* https://pandas.pydata.org/docs/reference/api/pandas.cut.html\n",
    "* https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html\n",
    "* Confirm with `pol_clm_db.info()` that after applying the needed transformations you do have:\n",
    "    * 3 continuous features (`Area`, `BonusMalus`, `log-Density`)\n",
    "    * 1 binary feature (`VehGas`)\n",
    "    * 5 categorical features (`VehPower`, `VehAge`, `DrivAge`, `VehBrand`, `Region`) with their corresponding levels or categories. You can use `pol_clm_db.VARIABLE.unique()` to check if the values listed are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "d5WY0BjtLROv"
   },
   "outputs": [],
   "source": [
    "pol_clm_db = pd.read_csv('https://katrienantonio.github.io/APC_pricing_case/datasets/pol_clm_db.csv', dtype = {'IDpol': int},\n",
    "   usecols = ['IDpol', 'ClaimNb', 'Exposure', 'Area', 'VehPower', 'VehAge', 'DrivAge', 'BonusMalus', 'VehBrand', 'VehGas', 'LogDensity', 'Region', 'ClaimAmount'],\n",
    "                         index_col = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_DrivAge(age):\n",
    "    if age < 21: \n",
    "        return '[18; 21)'  \n",
    "    elif age < 26:\n",
    "        return '[21; 26)'\n",
    "    elif age < 31:\n",
    "        return ' [26; 31)' \n",
    "    elif age < 41:\n",
    "        return '[31; 41)'\n",
    "    elif age < 51:\n",
    "        return '[41; 51)'\n",
    "    elif age < 71:\n",
    "        return '[51; 71)'\n",
    "    else: \n",
    "        return '[71;+inf)'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehbrand_dict = dict()\n",
    "region_dict = dict()\n",
    "\n",
    "for i in range(1,15):\n",
    "    vehbrand_dict['B' + str(i)] = i\n",
    "    \n",
    "for i in range(1,23):\n",
    "    vehbrand_dict['R' + str(i)] = i\n",
    "\n",
    "gas_dict = {'Regular': 0, 'Diesel': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 678013 entries, 0 to 678012\n",
      "Data columns (total 22 columns):\n",
      " #   Column         Non-Null Count   Dtype   \n",
      "---  ------         --------------   -----   \n",
      " 0   IDpol          678013 non-null  int32   \n",
      " 1   ClaimNb        678013 non-null  int64   \n",
      " 2   Exposure       678013 non-null  float64 \n",
      " 3   Area           678013 non-null  int64   \n",
      " 4   VehPower       678013 non-null  float64 \n",
      " 5   VehAge         678013 non-null  float64 \n",
      " 6   DrivAge        678013 non-null  float64 \n",
      " 7   BonusMalus     678013 non-null  float64 \n",
      " 8   VehBrand       678013 non-null  object  \n",
      " 9   VehGas         678013 non-null  object  \n",
      " 10  Region         678013 non-null  object  \n",
      " 11  ClaimAmount    678013 non-null  float64 \n",
      " 12  LogDensity     678013 non-null  float64 \n",
      " 13  AreaGLM        678013 non-null  float64 \n",
      " 14  VehPowerGLM    678013 non-null  category\n",
      " 15  VehAgeGLM      678013 non-null  category\n",
      " 16  DrivAgeGLM     678013 non-null  category\n",
      " 17  BonusMalusGLM  678013 non-null  float64 \n",
      " 18  VehBrandGLM    678013 non-null  category\n",
      " 19  VehGasGLM      678013 non-null  category\n",
      " 20  LogDensityGLM  678013 non-null  float64 \n",
      " 21  RegionGLM      678013 non-null  category\n",
      "dtypes: category(6), float64(10), int32(1), int64(2), object(3)\n",
      "memory usage: 84.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Perform the needed transformations according to the type and description of the variable\n",
    "pol_clm_db['AreaGLM']         = pol_clm_db['Area'].astype('float') #nakijken\n",
    "pol_clm_db['VehPowerGLM']     = pol_clm_db['Area'].astype('category')\n",
    "pol_clm_db['VehAgeGLM']       = pol_clm_db['VehAge'].apply(lambda x: '[0,1)' if x < 1 else ('[1,10)' if x < 10 else '[10, inf)')).astype('category')\n",
    "pol_clm_db['DrivAgeGLM']      = pol_clm_db['DrivAge'].apply(map_DrivAge).astype('category')\n",
    "pol_clm_db['BonusMalusGLM']   = pol_clm_db['BonusMalus'].clip(upper=150)\n",
    "pol_clm_db['VehBrandGLM']     = pol_clm_db['VehBrand'].astype('category')\n",
    "pol_clm_db['VehGasGLM']       = pol_clm_db['VehGas'].astype('category')\n",
    "pol_clm_db['LogDensityGLM']   = pol_clm_db['LogDensity']\n",
    "pol_clm_db['RegionGLM']       = pol_clm_db['Region'].astype(\"category\")\n",
    "\n",
    "pol_clm_db.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzhTylxCLROv"
   },
   "source": [
    "#### 1.2. Responses pre-processing\n",
    "\n",
    "游닇 Create the following response variables:\n",
    "* Frequency: number of claims by exposure\n",
    "\n",
    "(be careful: in claim count models we either use the number of claims as outcome variable combined with exposure, or the frequency variable with exposure as weight in the loss function; in the below instructions carefully reflect if you want to use frequency or number of claims)\n",
    "* Severity: average claim amount per claim reported\n",
    "* Pure premium: claim amount per unit of exposure\n",
    "\n",
    "游닇 Check the existence of missing values for any of these response variables\n",
    "\n",
    "游닉 Hint:\n",
    "\n",
    "* Be careful when calculating the severity as there are (obviously) many cases where the number of claims reported is 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0z9uFt0eLROv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDpol                 0\n",
      "ClaimNb               0\n",
      "Exposure              0\n",
      "Area                  0\n",
      "VehPower              0\n",
      "VehAge                0\n",
      "DrivAge               0\n",
      "BonusMalus            0\n",
      "VehBrand              0\n",
      "VehGas                0\n",
      "Region                0\n",
      "ClaimAmount           0\n",
      "LogDensity            0\n",
      "AreaGLM               0\n",
      "VehPowerGLM           0\n",
      "VehAgeGLM             0\n",
      "DrivAgeGLM            0\n",
      "BonusMalusGLM         0\n",
      "VehBrandGLM           0\n",
      "VehGasGLM             0\n",
      "LogDensityGLM         0\n",
      "RegionGLM             0\n",
      "Frequency             0\n",
      "Severity         643953\n",
      "PurePremium           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "pol_clm_db['Frequency']   = pol_clm_db['ClaimNb']/pol_clm_db['Exposure']\n",
    "pol_clm_db[\"Severity\"]    = pol_clm_db['ClaimAmount']/pol_clm_db['ClaimNb']\n",
    "pol_clm_db[\"PurePremium\"] = pol_clm_db['ClaimAmount']/pol_clm_db['Exposure']\n",
    "\n",
    "# check missing value\n",
    "print(pol_clm_db.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDpol            0\n",
      "ClaimNb          0\n",
      "Exposure         0\n",
      "Area             0\n",
      "VehPower         0\n",
      "VehAge           0\n",
      "DrivAge          0\n",
      "BonusMalus       0\n",
      "VehBrand         0\n",
      "VehGas           0\n",
      "Region           0\n",
      "ClaimAmount      0\n",
      "LogDensity       0\n",
      "AreaGLM          0\n",
      "VehPowerGLM      0\n",
      "VehAgeGLM        0\n",
      "DrivAgeGLM       0\n",
      "BonusMalusGLM    0\n",
      "VehBrandGLM      0\n",
      "VehGasGLM        0\n",
      "LogDensityGLM    0\n",
      "RegionGLM        0\n",
      "Frequency        0\n",
      "Severity         0\n",
      "PurePremium      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "pol_clm_db['Severity'].fillna(0, inplace = True)\n",
    "print(pol_clm_db.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pc5Dn2ATLROw",
    "tags": []
   },
   "source": [
    "#### 1.3. Train and test sets\n",
    "\n",
    "游닇 Use the `train_test_split` function from the module `sklearn.model_selection` to split the data into training (90%) and test sets(10%)\n",
    "* Fix the seed `random_state=210`\n",
    "\n",
    "游닇 Study and run the function `split_stat`\n",
    "\n",
    "游닇 Apply `split_stat` to check if the resulting split for test and train data looks reasonable (similar) in terms of exposure, claim numbers, frequency and severity.\n",
    "\n",
    "What can you conclude from the results?\n",
    "\n",
    "游닉 Hint:\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vBl4rpIXLROw"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDpol</th>\n",
       "      <th>ClaimNb</th>\n",
       "      <th>Exposure</th>\n",
       "      <th>Area</th>\n",
       "      <th>VehPower</th>\n",
       "      <th>VehAge</th>\n",
       "      <th>DrivAge</th>\n",
       "      <th>BonusMalus</th>\n",
       "      <th>VehBrand</th>\n",
       "      <th>VehGas</th>\n",
       "      <th>...</th>\n",
       "      <th>VehAgeGLM</th>\n",
       "      <th>DrivAgeGLM</th>\n",
       "      <th>BonusMalusGLM</th>\n",
       "      <th>VehBrandGLM</th>\n",
       "      <th>VehGasGLM</th>\n",
       "      <th>LogDensityGLM</th>\n",
       "      <th>RegionGLM</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Severity</th>\n",
       "      <th>PurePremium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>583250</th>\n",
       "      <td>4174428</td>\n",
       "      <td>0</td>\n",
       "      <td>0.72</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>B2</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>...</td>\n",
       "      <td>[10, inf)</td>\n",
       "      <td>[31; 41)</td>\n",
       "      <td>50.0</td>\n",
       "      <td>B2</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>4.0</td>\n",
       "      <td>R31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164543</th>\n",
       "      <td>1146791</td>\n",
       "      <td>0</td>\n",
       "      <td>0.93</td>\n",
       "      <td>3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>B10</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>...</td>\n",
       "      <td>[1,10)</td>\n",
       "      <td>[26; 31)</td>\n",
       "      <td>57.0</td>\n",
       "      <td>B10</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>5.0</td>\n",
       "      <td>R52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608939</th>\n",
       "      <td>5032348</td>\n",
       "      <td>0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>B12</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>...</td>\n",
       "      <td>[1,10)</td>\n",
       "      <td>[51; 71)</td>\n",
       "      <td>50.0</td>\n",
       "      <td>B12</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>4.0</td>\n",
       "      <td>R22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420860</th>\n",
       "      <td>3130884</td>\n",
       "      <td>0</td>\n",
       "      <td>0.83</td>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>B11</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>...</td>\n",
       "      <td>[1,10)</td>\n",
       "      <td>[51; 71)</td>\n",
       "      <td>50.0</td>\n",
       "      <td>B11</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>5.0</td>\n",
       "      <td>R24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423281</th>\n",
       "      <td>3135372</td>\n",
       "      <td>0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>B1</td>\n",
       "      <td>Regular</td>\n",
       "      <td>...</td>\n",
       "      <td>[1,10)</td>\n",
       "      <td>[26; 31)</td>\n",
       "      <td>60.0</td>\n",
       "      <td>B1</td>\n",
       "      <td>Regular</td>\n",
       "      <td>9.0</td>\n",
       "      <td>R82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows 칑 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          IDpol  ClaimNb  Exposure  Area  VehPower  VehAge  DrivAge  \\\n",
       "583250  4174428        0      0.72     2       6.0    17.0     36.0   \n",
       "164543  1146791        0      0.93     3       7.0     6.0     28.0   \n",
       "608939  5032348        0      0.06     2       8.0     2.0     53.0   \n",
       "420860  3130884        0      0.83     3       9.0     2.0     51.0   \n",
       "423281  3135372        0      0.16     5       7.0     2.0     28.0   \n",
       "\n",
       "        BonusMalus VehBrand   VehGas  ...  VehAgeGLM  DrivAgeGLM  \\\n",
       "583250        50.0       B2   Diesel  ...  [10, inf)    [31; 41)   \n",
       "164543        57.0      B10   Diesel  ...     [1,10)    [26; 31)   \n",
       "608939        50.0      B12   Diesel  ...     [1,10)    [51; 71)   \n",
       "420860        50.0      B11   Diesel  ...     [1,10)    [51; 71)   \n",
       "423281        60.0       B1  Regular  ...     [1,10)    [26; 31)   \n",
       "\n",
       "        BonusMalusGLM  VehBrandGLM VehGasGLM LogDensityGLM RegionGLM  \\\n",
       "583250           50.0           B2    Diesel           4.0       R31   \n",
       "164543           57.0          B10    Diesel           5.0       R52   \n",
       "608939           50.0          B12    Diesel           4.0       R22   \n",
       "420860           50.0          B11    Diesel           5.0       R24   \n",
       "423281           60.0           B1   Regular           9.0       R82   \n",
       "\n",
       "        Frequency Severity PurePremium  \n",
       "583250        0.0      0.0         0.0  \n",
       "164543        0.0      0.0         0.0  \n",
       "608939        0.0      0.0         0.0  \n",
       "420860        0.0      0.0         0.0  \n",
       "423281        0.0      0.0         0.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "trn_db, tst_db = train_test_split(pol_clm_db, test_size=0.1,  random_state=210, shuffle=True)\n",
    "\n",
    "tst_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "kA1b7FLqLROw"
   },
   "outputs": [],
   "source": [
    "# Check datasets\n",
    "def split_stat(dat_in, trn_db, tst_db):\n",
    "    # dat_in: full database\n",
    "    # trn_db: train database\n",
    "    # tst_db: test database\n",
    "\n",
    "    # Check resulting split looks reasonable\n",
    "    return dat_in.assign(\n",
    "        # Add indicator columns for whether each row is in_train, in_test\n",
    "        in_train=dat_in.IDpol.isin(trn_db.IDpol),\n",
    "        in_test=dat_in.IDpol.isin(tst_db.IDpol),\n",
    "        # Add column of which subset each row is in\n",
    "        subset=lambda x: np.select(\n",
    "            [x.in_train, x.in_test],\n",
    "            ['train', 'test'],\n",
    "            default='no_subset')\n",
    "    ).groupby(  # Group rows by which subset they are in\n",
    "        ['in_train', 'in_test', 'subset']\n",
    "    ).agg({  # Calculate stats for each group\n",
    "        'IDpol': 'size', 'Exposure': 'sum', 'ClaimNb': 'sum', 'ClaimAmount': 'sum'\n",
    "    }).rename(columns={'IDpol': 'NumOfRows'}).assign(\n",
    "        # Add additional stats\n",
    "        NumOfRowsProp = lambda x: x.NumOfRows   / x.NumOfRows.sum(),\n",
    "        ExposureProp  = lambda x: x.Exposure    / x.Exposure.sum(),\n",
    "        ClaimNbProp   = lambda x: x.ClaimNb     / x.ClaimNb.sum(),\n",
    "        Frequency     = lambda x: x.ClaimNb     / x.Exposure,\n",
    "        Severity      = lambda x: x.ClaimAmount / x.ClaimNb\n",
    "\n",
    "    ).pipe(lambda df: df.append(pd.DataFrame.from_dict({\n",
    "        # Add totals row. It is the sum for every column except 'Frequency', 'Severity'\n",
    "        # where it is the overall claims frequency of the entire data set\n",
    "        ('Total','','',''): [\n",
    "            (lambda x: df.ClaimNb.sum() / df.Exposure.sum() if x == 'Frequency' else df.ClaimAmount.sum() / df.ClaimNb.sum())(col_name) if col_name in ['Frequency', 'Severity']\n",
    "            else df.loc[:,col_name].sum() for col_name in df.columns\n",
    "        ]}, orient='index', columns=df.columns\n",
    "    ))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "c_Xq7V7QLROw"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>NumOfRows</th>\n",
       "      <th>Exposure</th>\n",
       "      <th>ClaimNb</th>\n",
       "      <th>ClaimAmount</th>\n",
       "      <th>NumOfRowsProp</th>\n",
       "      <th>ExposureProp</th>\n",
       "      <th>ClaimNbProp</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Severity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in_train</th>\n",
       "      <th>in_test</th>\n",
       "      <th>subset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>test</th>\n",
       "      <td>67802</td>\n",
       "      <td>35853.301273</td>\n",
       "      <td>3521</td>\n",
       "      <td>5541119.41</td>\n",
       "      <td>0.100001</td>\n",
       "      <td>0.100048</td>\n",
       "      <td>0.097654</td>\n",
       "      <td>0.098206</td>\n",
       "      <td>1573.734567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th>False</th>\n",
       "      <th>train</th>\n",
       "      <td>610211</td>\n",
       "      <td>322506.804190</td>\n",
       "      <td>32535</td>\n",
       "      <td>54368097.09</td>\n",
       "      <td>0.899999</td>\n",
       "      <td>0.899952</td>\n",
       "      <td>0.902346</td>\n",
       "      <td>0.100882</td>\n",
       "      <td>1671.064917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>678013</td>\n",
       "      <td>358360.105463</td>\n",
       "      <td>36056</td>\n",
       "      <td>59909216.50</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100614</td>\n",
       "      <td>1661.560253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         NumOfRows       Exposure  ClaimNb  ClaimAmount  \\\n",
       "in_train in_test subset                                                   \n",
       "False    True    test        67802   35853.301273     3521   5541119.41   \n",
       "True     False   train      610211  322506.804190    32535  54368097.09   \n",
       "Total                       678013  358360.105463    36056  59909216.50   \n",
       "\n",
       "                         NumOfRowsProp  ExposureProp  ClaimNbProp  Frequency  \\\n",
       "in_train in_test subset                                                        \n",
       "False    True    test         0.100001      0.100048     0.097654   0.098206   \n",
       "True     False   train        0.899999      0.899952     0.902346   0.100882   \n",
       "Total                         1.000000      1.000000     1.000000   0.100614   \n",
       "\n",
       "                            Severity  \n",
       "in_train in_test subset               \n",
       "False    True    test    1573.734567  \n",
       "True     False   train   1671.064917  \n",
       "Total                    1661.560253  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_stat(pol_clm_db, trn_db, tst_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFkh8vhkLROw",
    "tags": []
   },
   "source": [
    "## 2.  Frequency modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EL5h3GXrLROw"
   },
   "source": [
    "\n",
    "From the previous task, we observe a slight bias in terms of frequency and severity across the two constructed data sets, which could be further analyzed w.r.t. the available features (i.e. whether we also have a feature shift), and one could also consider the construction of a stratified choice of training and test data sets. Here, we refrain from doing so.\n",
    "\n",
    "We can now kick off the modeling part. In the following, we will fit various claim frequency models based on a Poisson distributional assumption.\n",
    "\n",
    "All insurance policies i = 1, 2, ... can be described by independent claim counts $N_i$ having distribution\n",
    "\n",
    "$$ N_i \\sim \\text{Poi} (\\lambda(x_i) \\cdot \\nu_i) $$.\n",
    "\n",
    "That is: claim counts $N_i$ are assumed to be independent across the policies $i=1, 2,\\ldots $ and to follow a Poisson distribution with mean $\\lambda(x_i) \\cdot \\nu_i $ where $\\nu_i$ is the registered exposure to risk for policyholder $i$ and $\\lambda(x_i)$ is a function of the covariates $x_i$.\n",
    "\n",
    "We will explore the following set of models:\n",
    "* Flat Load\n",
    "* GLM\n",
    "* Regression Tree\n",
    "* Random Forest\n",
    "* GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAJC6qR0LROw"
   },
   "source": [
    "#### 2.1. The Poisson deviance\n",
    "\n",
    "The (scaled) Poisson deviance is defined as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "D^{*}(\\boldsymbol{N}, \\lambda) &=\\sum_{i=1}^{n} 2 N_{i}\\left[\\frac{\\lambda(x_i) v_{i}}{N_{i}}-1-\\log \\left(\\frac{\\lambda(x_i) v_{i}}{N_{i}}\\right)\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $N_i$ is the observed number of claims, $v_i$ is the exposure, and the $i^{th}$ term is set equal to $2\\lambda(x_i) v_i$ for $N_i = 0$.\n",
    "\n",
    "For fair comparison of results, we define **average poisson deviance loss** as (scaled) Poisson deviance divided by the number of observatons in training or test set. Denote the number of observations with $n$, then we want\n",
    "\n",
    "$$\\frac{D^{*}(\\boldsymbol{N}, \\lambda)}{n} $$\n",
    "\n",
    "游닇 Compare the given expression for the deviance to the one discussed in the lectures, and expressions found in the literature.\n",
    "\n",
    "游닇 Define a function `av_poisson_deviance` for calculating the average poisson deviance (as defined above), for training and test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_C9Gl4epLROw"
   },
   "outputs": [],
   "source": [
    "# As we are going to compare various models, we create a table which stores the metrics we are going to use for the comparison and the selection of the best model.\n",
    "\n",
    "mod_res = pd.DataFrame(\n",
    "    {'model'             : pd.Series(dtype='str'),\n",
    "     'in_sample_loss'    : pd.Series(dtype='float'),\n",
    "     'out_sample_loss'   : pd.Series(dtype='float'),\n",
    "     'aic'               : pd.Series(dtype='float'),\n",
    "     'in_sample_gini'    : pd.Series(dtype='float'),\n",
    "     'out_sample_gini'   : pd.Series(dtype='float'),\n",
    "     'number_of_param'   : pd.Series(dtype='int')\n",
    "     })\n",
    "\n",
    "\n",
    "# av_poisson_deviance: average possion deviance, which is defined as scaled possion deviance divided by the number of observations\n",
    "def av_poisson_deviance(y_freq, p_freq, exposure):\n",
    "    # y_freq: array-like, observed frequency\n",
    "    # p_freq: array-like, predicted number of claims\n",
    "    # exposure: array-like, exposure\n",
    "\n",
    "    y_freq, p_freq = np.asarray(y_freq), np.asarray(p_freq)\n",
    "    exposure = np.asarray(exposure)\n",
    "    \n",
    "    mask = y_freq == 0\n",
    "    y_freq[mask] = (p_freq * exposure)[mask]\n",
    "    \n",
    "    y = y_freq   # observed number of claims\n",
    "    p = p_freq   # predicted number of claims\n",
    "    \n",
    "    d = 2*np.sum(exposure*p_freq/y_freq - 1 - np.log (exposure*p_freq/y_freq))\n",
    "\n",
    "    av_deviance = d/len(y_freq)\n",
    "\n",
    "    return(av_deviance)\n",
    "\n",
    "y_freq_trn   = trn_db['Frequency']\n",
    "y_trn        = trn_db['ClaimNb']\n",
    "expo_trn     = trn_db['Exposure']\n",
    "\n",
    "y_freq_tst   = tst_db['Frequency']\n",
    "y_tst        = tst_db['ClaimNb']\n",
    "expo_tst     = tst_db['Exposure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "1VwN3tPqLROx"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "def lorenz_curve(y_freq, p_freq, exposure):\n",
    "    '''\n",
    "    y_freq: actual frequency\n",
    "    p_freq: predicated frequency\n",
    "    exposure: exposure\n",
    "\n",
    "    '''\n",
    "\n",
    "    y_freq, p_freq = np.asarray(y_freq), np.asarray(p_freq)\n",
    "    exposure = np.asarray(exposure)\n",
    "\n",
    "    # order samples by increasing predicted risk:\n",
    "    ranking = np.argsort(p_freq)\n",
    "    ranked_frequencies = y_freq[ranking]\n",
    "    ranked_exposure = exposure[ranking]\n",
    "    cumulated_claims = np.cumsum(ranked_frequencies * ranked_exposure)\n",
    "    cumulated_claims /= cumulated_claims[-1]\n",
    "    cumulated_exposure = np.cumsum(ranked_exposure)\n",
    "    cumulated_exposure /= cumulated_exposure[-1]\n",
    "\n",
    "    gini = 1 - 2 * auc(cumulated_exposure, cumulated_claims)\n",
    "\n",
    "    return cumulated_exposure, cumulated_claims, gini\n",
    "\n",
    "def store_mod_res(\n",
    "    mod_res,\n",
    "    mod,\n",
    "    mod_name,\n",
    "    y_freq_trn, p_freq_trn, expo_trn,\n",
    "    y_freq_tst, p_freq_tst, expo_tst,\n",
    "    isGLM=True,\n",
    "    isRF=False,\n",
    "    isGBM=False):\n",
    "\n",
    "    n = len(mod_res)\n",
    "\n",
    "    trn_av_d = av_poisson_deviance(y_freq_trn, p_freq_trn, expo_trn)\n",
    "    tst_av_d = av_poisson_deviance(y_freq_tst, p_freq_tst, expo_tst)\n",
    "\n",
    "    # trn_av_d = mean_poisson_deviance(y_freq_trn, p_freq_trn, expo_trn)\n",
    "    # tst_av_d = mean_poisson_deviance(y_freq_tst, p_freq_tst, expo_tst)\n",
    "\n",
    "    AIC      = round(mod.aic,0) if isGLM else np.nan\n",
    "\n",
    "    if isGLM:\n",
    "        param_n  = len(mod.params)\n",
    "    elif isRF:\n",
    "        param_n =  mod.n_features_in_\n",
    "    elif not isGBM:\n",
    "        param_n = mod.get_n_leaves()\n",
    "    else:\n",
    "        param_n = np.nan\n",
    "\n",
    "    _, _, trn_gini = lorenz_curve(y_freq_trn, p_freq_trn, expo_trn)\n",
    "    _, _, tst_gini = lorenz_curve(y_freq_tst, p_freq_tst, expo_tst)\n",
    "\n",
    "    mod_res.loc[n] = [mod_name, trn_av_d, tst_av_d, AIC, trn_gini, tst_gini, param_n]\n",
    "\n",
    "    return mod_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nG1pFPJYLROx",
    "tags": []
   },
   "source": [
    "#### 2.2. Naive GLM\n",
    "\n",
    "We are now ready to construct a first frequency model using a naive GLM, that is a GLM in which **no variables are included**. This is also called a flat load model.\n",
    "\n",
    "游닇Complete the code for the naive GLM; be careful with the combination of response variable + exposure, as mentioned above.\n",
    "\n",
    "游닇Get the fitted values from the trained GLM for both train and test sets and run the `store_mod_res` to store the model results\n",
    "\n",
    "游닇Verify that the fitted value equals the observed claim frequency (per unit of exposure) on the train dataset\n",
    "\n",
    "游닉 Hints:\n",
    "* https://www.statsmodels.org/stable/glm.html\n",
    "* The formula for a model with no variables follows as `response_variable ~ 1`\n",
    "* https://numpy.org/doc/stable/reference/generated/numpy.exp.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ree4VAR_LROx"
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "# Build the homogeneous GLM\n",
    "GLM_freq_naive = smf.glm(\n",
    "  ____________,\n",
    "  data = trn_db,\n",
    "  exposure = np.asarray(trn_db['Exposure']),\n",
    "  family = sm.families.Poisson(sm.genmod.families.links.log())\n",
    ").fit()\n",
    "\n",
    "print(GLM_freq_naive.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2GMPX3yJLROx"
   },
   "outputs": [],
   "source": [
    "# Get fitted values for train and test sets\n",
    "p_freq_trn = np.repeat(____________, len(trn_db))\n",
    "p_freq_tst_GLM0 = np.repeat(____________, len(tst_db))\n",
    "\n",
    "# Get assessment metrics\n",
    "mod_res = store_mod_res(mod_res, GLM_freq_naive, \"GLM_freq_naive\", y_freq_trn, p_freq_trn, expo_trn, y_freq_tst, p_freq_tst_GLM0, expo_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2etYpzIhLROx"
   },
   "outputs": [],
   "source": [
    "print( 'GLM estimates:', round( np.exp(GLM_freq_homo.params[0]), 4))\n",
    "print( 'Average frequency on the train dataset:', round( sum( trn_db['ClaimNb'] )/sum( trn_db['Exposure'] ), 4) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vj0xbF0kLROx",
    "tags": []
   },
   "source": [
    "#### 2.3. GLM with all the features\n",
    "\n",
    "游닇Complete the code for a GLM with the 9 covariates included\n",
    "\n",
    "游닇Get the fitted values from the trained GLM for both train and test sets and run the `store_mod_res` to store the model results; reflect on these metrics: what is stored here?\n",
    "\n",
    "游닇Inspect the model summary\n",
    "\n",
    "游눫 What can you conclude from the results obtained?\n",
    "\n",
    "Would you perform any changes wrt the model specification? If so, what are your suggestions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aTUJWdHNLROx"
   },
   "outputs": [],
   "source": [
    "features = ____________   # list with 9 text entries (variables' names)\n",
    "GLM_freq_all = smf.glm(\n",
    "  \"ClaimNb ~ \" + ' + '.join(features),\n",
    "  data=____________,      # train database\n",
    "  exposure=____________,  # exposure array\n",
    "  family=sm.families.Poisson(sm.genmod.families.links.log())\n",
    ").fit()\n",
    "\n",
    "# Get estimates\n",
    "p_freq_trn = ____________ # train database\n",
    "p_freq_tst_GLM = ____________ # test database\n",
    "\n",
    "mod_res = store_mod_res(mod_res, GLM_freq_all, \"GLM_freq_all\", y_freq_trn, p_freq_trn, expo_trn, p_freq_tst_GLM, p_freq_tst_GLM0, expo_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HaH7_4q5LROx"
   },
   "outputs": [],
   "source": [
    "# Inspect the model summary\n",
    "print(GLM_freq_all.summary())\n",
    "mod_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KElkOLZalWue"
   },
   "source": [
    "### 2.4. A regression tree with binary splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIt9-fa1LROx",
    "tags": []
   },
   "source": [
    "##### 2.4.1. Fitting a regression tree\n",
    "\n",
    "We propose the following pre-processing steps for the regression trees:\n",
    "\n",
    "1. If there is a natural ordering in a categorical feature, then we will replace this feature with an increasing sequence of real numbers\n",
    "\n",
    "2. It can be computationally very expensive if we have many (unordered) categorical feature components with many possible levels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9wLQ5JxLROx"
   },
   "source": [
    "\n",
    "\n",
    "游닇 Define two lists with continuous and categorical features, respectively\n",
    "\n",
    "游닇 Run the one hot encoding on the categorical features. Explain what this does.\n",
    "\n",
    "游닇 Define the response variables for both train and test datasets\n",
    "\n",
    "游닇 Which of the 4 options below is the correct instruction to fit the regression tree?\n",
    "\n",
    "    1. DecisionTree.fit(X_trn, ClaimNb, sample_weight=None)\n",
    "\n",
    "    2. DecisionTree.fit(X_trn, ClaimNb, sample_weight=Exposure)\n",
    "\n",
    "    3. DecisionTree.fit(X_trn, Frequency, sample_weight=None)\n",
    "\n",
    "    4. DecisionTree.fit(X_trn, Frequency, sample_weight=Exposure)\n",
    "\n",
    "Implement the correct one and fit the model. Reflect on control parameters that can be set. Reflect on the possibility of overfitting and ways to combat overfitting when constructing a regression tree.\n",
    "\n",
    "游닇 Get the fitted values from the trained model for both train and test sets and run the `store_mod_res` to store the model results\n",
    "\n",
    "游닉 Hints:\n",
    "* https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VV6pE45_LROy"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "feat_cnt = ____________ # list with (3) continuous features\n",
    "feat_cat  = ____________ # list with (6) categorical features\n",
    "\n",
    "# one hot encoding for categorical variables as DecisionTreeRegressor can only deal with numerical values\n",
    "# https://www.educative.io/blog/one-hot-encoding\n",
    "onehot_coded_trn_db = pd.get_dummies(trn_db[feat_cat])\n",
    "onehot_coded_tst_db = pd.get_dummies(tst_db[feat_cat])\n",
    "\n",
    "X_trn = pd.concat([trn_db[feat_cnt], onehot_coded_trn_db], axis=1)\n",
    "X_tst = pd.concat([tst_db[feat_cnt], onehot_coded_tst_db], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ttshQRGLROy"
   },
   "outputs": [],
   "source": [
    "# Set response variable: frequency\n",
    "y_trn = ____________   # on train dataset\n",
    "y_tst = ____________   # on test dataset\n",
    "\n",
    "# criterion = poisson in DecisionTree seems not work very well. So in this case, we use default loss function to split the tree. The result can be regarded as a good approximation to the optimal split as long as data size is large enough.\n",
    "RT0_freq = DecisionTreeRegressor(ccp_alpha=5e-5, min_samples_leaf=10_000, random_state=2000)\n",
    "\n",
    "# What of the 4 formulas is the correct?\n",
    "RT0_freq.fit(X_trn,                     # train dataset\n",
    "    y_trn,                              # response variable\n",
    "    sample_weight = ____________        # weight array\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84TVlT78LROy"
   },
   "outputs": [],
   "source": [
    "# Get estimates and inspect them\n",
    "p_freq_trn     = RT0_freq.predict(____________) # train dataset\n",
    "p_freq_tst_RT0 = RT0_freq.predict(____________) # test dataset\n",
    "\n",
    "p_freq_trn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y6rhELHcLROy"
   },
   "outputs": [],
   "source": [
    "# Get model results\n",
    "mod_res = store_mod_res(mod_res, RT0_freq, \"RT_freq\", y_freq_trn, p_freq_trn, expo_trn, y_freq_tst, p_freq_tst_RT0, expo_tst, isGLM=False)\n",
    "mod_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jiaz4UfwLROy"
   },
   "source": [
    "##### 2.4.2. Visualize the regression tree\n",
    "\n",
    "Python offers tools to nicely visualize the constructed trees.\n",
    "\n",
    "游닇 Investigate the function `plot_tree` from `sklearn`\n",
    "\n",
    "游닇 Implement the `plot_tree` from `sklearn` to obtain the interpretation of the model. What is your understanding?\n",
    "\n",
    "游닇 Run the partial dependence plot for the `BonusMalusGLM`. What can you conclude?\n",
    "\n",
    "游닉 Hints:\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_text.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1K92L2WLROy"
   },
   "outputs": [],
   "source": [
    "# Decision tree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "# Tree visualization\n",
    "plt.figure(figsize=(10,8), dpi=150)\n",
    "plot_tree(RT0_freq, feature_names=X_trn.columns, proportion=True)\n",
    "\n",
    "from sklearn.tree import export_text\n",
    "r = export_text(RT0_freq, feature_names=X_trn.columns.tolist())\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "diBVqtfrLROy"
   },
   "outputs": [],
   "source": [
    "# The partial dependence plot shows the marginal effect one or two features have on the predicted outcome of a machine learning model.\n",
    "# A partial dependence plot can show whether the relationship between the target and a feature is linear, monotonic or more complex. For example, when applied to a linear regression model, partial dependence plots always show a linear relationship.\n",
    "# E(Y | X=x)\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "PartialDependenceDisplay.from_estimator(RT0_freq, X_trn, ['BonusMalusGLM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHS_3FmXLROy",
    "tags": []
   },
   "source": [
    "### 2.5. Random Forest\n",
    "\n",
    "游닇 Complete the blank spaces (similar to the ones already completed in Section 2.5 on Regression Tree) and run the code\n",
    "\n",
    "游닇 Run the cell to obtain the model results. What is the best model so far?\n",
    "\n",
    "\n",
    "游닇 Run the partial dependence plot for the `BonusMalusGLM`. What can you conclude?\n",
    "\n",
    "游닉 Hints:\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "vPNNEGinLROy"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "feat_cnt = ['Area', 'BonusMalusGLM', 'LogDensity'] # list with (3) continuous features\n",
    "feat_cat  = ['VehPowerGLM', 'VehAgeGLM', 'DrivAgeGLM', 'VehBrandGLM', 'RegionGLM'] # list with (6) categorical features\n",
    "\n",
    "\n",
    "# one hot encoding for categorical variables as DecisionTreeRegressor can only deal with numerical values\n",
    "# https://www.educative.io/blog/one-hot-encoding\n",
    "onehot_coded_trn_db = pd.get_dummies(trn_db[feat_cat])\n",
    "onehot_coded_tst_db = pd.get_dummies(tst_db[feat_cat])\n",
    "\n",
    "X_trn = pd.concat([trn_db[feat_cnt], onehot_coded_trn_db], axis=1)\n",
    "X_tst = pd.concat([tst_db[feat_cnt], onehot_coded_tst_db], axis=1)\n",
    "\n",
    "# Set response variable: frequency\n",
    "y_trn = trn_db['Frequency']   # train dataset\n",
    "y_tst = tst_db['Frequency']   # test dataset\n",
    "\n",
    "# criterion = poisson in DecisionTree seems not work very well. So in this case, we use default loss function to split the tree. The result can be regarded as a good approximation to the optimal split as long as data size is large enough.\n",
    "RF_freq = RandomForestRegressor(max_depth=2, random_state=0, criterion='poisson')\n",
    "\n",
    "RF_freq.fit(X_trn, # train dataset\n",
    "    y_trn,          # response variable\n",
    "    sample_weight=expo_trn\n",
    " )\n",
    "\n",
    "# Get estimates\n",
    "p_freq_trn     = RF_freq.predict(X_trn) # train dataset\n",
    "p_freq_tst_RF  = RF_freq.predict(X_tst) # test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "rTRBrK1nLROy"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>in_sample_loss</th>\n",
       "      <th>out_sample_loss</th>\n",
       "      <th>aic</th>\n",
       "      <th>in_sample_gini</th>\n",
       "      <th>out_sample_gini</th>\n",
       "      <th>number_of_param</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_freq</td>\n",
       "      <td>0.273098</td>\n",
       "      <td>0.270906</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.197028</td>\n",
       "      <td>0.203145</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF_freq</td>\n",
       "      <td>0.558230</td>\n",
       "      <td>0.557986</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.195617</td>\n",
       "      <td>0.201237</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lgbm_freq</td>\n",
       "      <td>0.570834</td>\n",
       "      <td>0.574071</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.247442</td>\n",
       "      <td>0.245874</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RF_freq</td>\n",
       "      <td>0.570834</td>\n",
       "      <td>0.557986</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.247442</td>\n",
       "      <td>0.201237</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RF_freq</td>\n",
       "      <td>0.570834</td>\n",
       "      <td>0.557986</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.247442</td>\n",
       "      <td>0.201237</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model  in_sample_loss  out_sample_loss  aic  in_sample_gini  \\\n",
       "0    RF_freq        0.273098         0.270906  NaN        0.197028   \n",
       "1    RF_freq        0.558230         0.557986  NaN        0.195617   \n",
       "2  lgbm_freq        0.570834         0.574071  NaN        0.247442   \n",
       "3    RF_freq        0.570834         0.557986  NaN        0.247442   \n",
       "4    RF_freq        0.570834         0.557986  NaN        0.247442   \n",
       "\n",
       "   out_sample_gini  number_of_param  \n",
       "0         0.203145             39.0  \n",
       "1         0.201237             52.0  \n",
       "2         0.245874              NaN  \n",
       "3         0.201237             52.0  \n",
       "4         0.201237             52.0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model results\n",
    "mod_res = store_mod_res(mod_res, RF_freq, \"RF_freq\", y_freq_trn, p_freq_trn, expo_trn, y_freq_tst, p_freq_tst_RF, expo_tst, isGLM=False, isRF=True)\n",
    "mod_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "jod9FVrNLROy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.inspection._plot.partial_dependence.PartialDependenceDisplay at 0x200a5201eb0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Partial dependency plot\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "PartialDependenceDisplay.from_estimator(RF_freq, X_trn, ['BonusMalusGLM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gu1elbTfLROz"
   },
   "source": [
    "### 2.6. Light GBM\n",
    "\n",
    "游닇 Complete the blank spaces and run the code\n",
    "\n",
    "游닇 Run the cell for the model results. What is the best model so far?\n",
    "\n",
    "游닉 Hints:\n",
    "* https://lightgbm.readthedocs.io/en/v3.3.2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Ra-FKHw7LROz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ik-ho\\AppData\\Local\\Temp/ipykernel_10940/1214400208.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_trn_lgbm[cat] = X_trn_lgbm[cat].cat.codes\n",
      "C:\\Users\\ik-ho\\AppData\\Local\\Temp/ipykernel_10940/1214400208.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_tst_lgbm[cat] = X_tst_lgbm[cat].cat.codes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008148 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 150\n",
      "[LightGBM] [Info] Number of data points in the train set: 610211, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score -1.774760\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "feat_cnt = ['Area', 'BonusMalusGLM', 'LogDensity'] # list with (3) continuous features\n",
    "feat_cat  = ['VehPowerGLM', 'VehAgeGLM', 'DrivAgeGLM', 'VehBrandGLM', 'RegionGLM'] # list with (6) categorical features\n",
    "\n",
    "\n",
    "# one hot encoding for categorical variables as DecisionTreeRegressor can only deal with numerical values\n",
    "# https://www.educative.io/blog/one-hot-encoding\n",
    "onehot_coded_trn_db = pd.get_dummies(trn_db[feat_cat])\n",
    "onehot_coded_tst_db = pd.get_dummies(tst_db[feat_cat])\n",
    "\n",
    "X_trn_lgbm = trn_db[feat_cnt + feat_cat]\n",
    "X_tst_lgbm = tst_db[feat_cnt + feat_cat]\n",
    "\n",
    "# Set response variable: frequency\n",
    "y_trn = trn_db['Frequency']   # train dataset\n",
    "y_tst = tst_db['Frequency']   # test dataset\n",
    "\n",
    "for cat in feat_cat:\n",
    "    X_trn_lgbm[cat] = X_trn_lgbm[cat].cat.codes\n",
    "    X_tst_lgbm[cat] = X_tst_lgbm[cat].cat.codes\n",
    "\n",
    "train_data = lgb.Dataset(X_trn_lgbm, label=y_trn, weight=expo_trn)\n",
    "\n",
    "# Define gbm\n",
    "param = {'objective': 'poisson'}\n",
    "param['metric'] = 'poisson'\n",
    "lgbm_freq = lgb.train(param, train_data)\n",
    "\n",
    "# Get estimates\n",
    "p_freq_trn      = lgbm_freq.predict(X_trn_lgbm) # train dataset\n",
    "p_freq_tst_LGBM = lgbm_freq.predict(X_tst_lgbm) # test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Vu14LoFELROz"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>in_sample_loss</th>\n",
       "      <th>out_sample_loss</th>\n",
       "      <th>aic</th>\n",
       "      <th>in_sample_gini</th>\n",
       "      <th>out_sample_gini</th>\n",
       "      <th>number_of_param</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_freq</td>\n",
       "      <td>0.273098</td>\n",
       "      <td>0.270906</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.197028</td>\n",
       "      <td>0.203145</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF_freq</td>\n",
       "      <td>0.558230</td>\n",
       "      <td>0.557986</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.195617</td>\n",
       "      <td>0.201237</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lgbm_freq</td>\n",
       "      <td>0.570834</td>\n",
       "      <td>0.574071</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.247442</td>\n",
       "      <td>0.245874</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model  in_sample_loss  out_sample_loss  aic  in_sample_gini  \\\n",
       "0    RF_freq        0.273098         0.270906  NaN        0.197028   \n",
       "1    RF_freq        0.558230         0.557986  NaN        0.195617   \n",
       "2  lgbm_freq        0.570834         0.574071  NaN        0.247442   \n",
       "\n",
       "   out_sample_gini  number_of_param  \n",
       "0         0.203145             39.0  \n",
       "1         0.201237             52.0  \n",
       "2         0.245874              NaN  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model results\n",
    "mod_res = store_mod_res(mod_res, lgbm_freq, \"lgbm_freq\", y_freq_trn, p_freq_trn, expo_trn, y_freq_tst, p_freq_tst_LGBM, expo_tst, isGLM=False, isRF=False, isGBM=True)\n",
    "mod_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Booster' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10940/3373701108.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlgbm_freq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Booster' object has no attribute 'summary'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCkpXG1cLROz"
   },
   "source": [
    "### 2.7. Final comparison\n",
    "\n",
    "游닇 Study the function `lift_chart`.\n",
    "* What is this function doing? What are the expected outputs?\n",
    "\n",
    "游닇 Run `lift_chart` to obtain the lift chart for the models tested\n",
    "* What can you conclude from these results? How do they relate with the observed in `mod_res`?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "0o4QS4Z9LRO0"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import gen_even_slices\n",
    "def mean_frequency_by_risk_group(y_true, y_pred, sample_weight=None, n_bins=100):\n",
    "    \"\"\"Compare predictions and observations for bins ordered by y_pred.\n",
    "\n",
    "    We order the samples by ``y_pred`` and split it in bins.\n",
    "    In each bin the observed mean is compared with the predicted mean.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: array-like of shape (n_samples,)\n",
    "        Ground truth (correct) target values.\n",
    "    y_pred: array-like of shape (n_samples,)\n",
    "        Estimated target values.\n",
    "    sample_weight : array-like of shape (n_samples,)\n",
    "        Sample weights.\n",
    "    n_bins: int\n",
    "        Number of bins to use.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bin_centers: ndarray of shape (n_bins,)\n",
    "        bin centers\n",
    "    y_true_bin: ndarray of shape (n_bins,)\n",
    "        average y_pred for each bin\n",
    "    y_pred_bin: ndarray of shape (n_bins,)\n",
    "        average y_pred for each bin\n",
    "    \"\"\"\n",
    "    idx_sort = np.argsort(y_pred)\n",
    "    bin_centers = np.arange(0, 1, 1 / n_bins) + 0.5 / n_bins\n",
    "    y_pred_bin = np.zeros(n_bins)\n",
    "    y_true_bin = np.zeros(n_bins)\n",
    "\n",
    "    for n, sl in enumerate(gen_even_slices(len(y_true), n_bins)):\n",
    "        weights = sample_weight[idx_sort][sl]\n",
    "        y_pred_bin[n] = np.average(y_pred[idx_sort][sl], weights=weights)\n",
    "        y_true_bin[n] = np.average(y_true[idx_sort][sl], weights=weights)\n",
    "    return bin_centers, y_true_bin, y_pred_bin\n",
    "\n",
    "def lift_chart(estimates_preds, estimates_names, y_tst, expo_tst):\n",
    "    \"\"\" Visual results from the mean_frequency_by_risk_group function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimates_preds: array-like with the models' predictions\n",
    "    # Example: estimates_preds = [p_freq_tst_GLM, p_freq_tst_RT0, p_freq_tst_RF, p_freq_tst_LGBM]\n",
    "    estimates_names: array-like with the models'names\n",
    "    # Example: estimates_names = ['GLM', 'Decision Tree', 'Random Forest', 'Light GBM']\n",
    "    expo_tst: array-like with the exposure\n",
    "    y_tst: array-like with observed values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    plot\n",
    "    \"\"\"\n",
    "\n",
    "    n_rows_plot = int(np.ceil(len(estimates_names)/2))\n",
    "    fig, ax = plt.subplots(nrows=n_rows_plot, ncols=2, figsize=(12, 8))\n",
    "    plt.subplots_adjust(wspace=0.3)\n",
    "    for axi, y_pred, model_name in zip(ax.ravel(), estimates_preds, estimates_names):\n",
    "        y_true = y_tst.values\n",
    "        exposure = expo_tst.values\n",
    "        y_pred = y_pred\n",
    "        q, y_true_seg, y_pred_seg = mean_frequency_by_risk_group(\n",
    "        y_true, y_pred, sample_weight=exposure, n_bins=10\n",
    "        )\n",
    "\n",
    "        axi.plot(q, y_pred_seg, marker=\"x\", linestyle=\"--\", label=\"predictions\")\n",
    "        axi.plot(q, y_true_seg, marker=\"o\", linestyle=\"--\", label=\"observations\")\n",
    "        axi.set_xlim(0, 1.0)\n",
    "        axi.set_ylim(0, 0.5)\n",
    "        axi.set(\n",
    "            title=model_name,\n",
    "            xlabel=\"Fraction of samples sorted by y_pred\",\n",
    "            ylabel=\"Mean Frequency (y_pred)\",\n",
    "        )\n",
    "        axi.legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kogdpIICLRO0"
   },
   "outputs": [],
   "source": [
    "lift_chart(['p_freq_tst_FL', 'p_freq_tst_GLM0', 'p_freq_tst_dt', 'p_freq_tst_RF', 'p_freq_tst_LGBM'], ['Flat Load', 'GLM', 'Decision Tree', 'Random Forest', 'Light GBM'], y_tst, expo_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0K5jujzLRO0"
   },
   "outputs": [],
   "source": [
    "mod_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBIYsDm0LRO0"
   },
   "source": [
    "### 2.8. Challenge 游끥\n",
    "\n",
    "Do you think it is possible to get **better models** than the 5 ones we went through?\n",
    "\n",
    "Yes, it is quite possible since we only modelled using barely the default values. There is obviously room for improvement!\n",
    "* One can try to tune any of the models trained so the depth, number of iterations/trees, number of features used, among others can be optimized\n",
    "* One can improve the tuning using $k$-fold cross validation instead of modelling directly on the whole train dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsauCkrsLRO0"
   },
   "source": [
    "Do you want to challenge the models created and come with a better solution?\n",
    "\n",
    "Then, take a look on the references hereunder and put your hands to work 游눩\n",
    "* https://lightgbm.readthedocs.io/en/v3.3.2/Parameters-Tuning.html\n",
    "* https://optuna.org/\n",
    "* https://github.com/microsoft/FLAML\n",
    "* ChatGPT\n",
    "\n",
    "\n",
    "To assess the model created you can use both functions  `store_mod_res` and `lift_chart`. Other metrics are also welcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUVjspajLRO0"
   },
   "source": [
    "游닇 Pick one model and explore at least one improvement in the fitting of this model. Carefully explain your strategy!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLqocyE7pcYK"
   },
   "source": [
    "## 3. Severity modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C97by1i6pu4w"
   },
   "source": [
    "游닇 You will now build some severity models as well, along the instructions outlined above:\n",
    "- a naive GLM for claim severity; which distributional assmption do you propose?\n",
    "- a GLM for claim severity, including features\n",
    "- a tree / RF or GBM for claim severity.\n",
    "\n",
    "Discuss your findings and observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UC8ce33qLRO0"
   },
   "source": [
    "## 游꿀 End of Part II of the pricing case!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "36ed71853c7621d18b70a5d8aacb70f40f1cb4d38722df4616345bc91a745d96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
